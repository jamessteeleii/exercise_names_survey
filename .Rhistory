alpha = 0.25) +
geom_line(data = zipf_preds,
aes(x=10^log10_rank, y=10^estimate, color = recognise),
linetype = "dashed") +
labs(x = "Term Frequency (proportion on log10 scale)",
y = "Term Rank (log10 scale)",
color = "Recognised\nExercise?") +
scale_color_brewer(palette = "Dark2") +
scale_x_log10() +
scale_y_log10() +
theme_bw()
# tf as proportion of all words for exercise (only recognised words)
exercise_words <- data_tokens |>
filter(recognise == "YES") |>
count(book_exercise_name, word, sort = TRUE)
total_words <- exercise_words |>
group_by(book_exercise_name) |>
summarize(total = sum(n))
exercise_words <- left_join(exercise_words, total_words)
ggplot(exercise_words, aes(n/total)) +
geom_histogram(show.legend = FALSE) +
facet_wrap("book_exercise_name", scales = "free") +
scale_y_continuous(trans = scales::pseudo_log_trans()) +
theme_bw()
ggsave("term_frequency_exercise.png", width = 10, height = 5, dpi = 300)
# Zipfs law
freq_by_rank <- exercise_words |>
group_by(book_exercise_name) |>
mutate(rank = row_number(),
tf = n/total,
log10_rank = log10(rank),
log10_tf = log10(tf)) |>
ungroup()
lm <- lm(log10_tf ~ lspline(log10_rank, 1) * book_exercise_name, data = freq_by_rank)
zipf_preds <- predictions(lm,
variables = list(book_exercise_name = unique(freq_by_rank$book_exercise_name)))
freq_by_rank |>
ggplot() +
geom_line(aes(x=rank, y=tf),
size = 1.1, alpha = 0.8, show.legend = FALSE) +
geom_ribbon(data = zipf_preds,
aes(x=10^log10_rank, ymin=10^conf.low, ymax=10^conf.high),
alpha = 0.25) +
geom_line(data = zipf_preds,
aes(x=10^log10_rank, y=10^estimate),
linetype = "dashed") +
labs(x = "Term Frequency (proportion on log10 scale)",
y = "Term Rank (log10 scale)") +
facet_wrap("book_exercise_name") +
scale_x_log10() +
scale_y_log10() +
theme_bw()
# tf as proportion of all words for exercise (both recognised and unrecognised words)
exercise_recognise_words <- data_tokens |>
count(book_exercise_name, recognise, word, sort = TRUE)
total_words <- exercise_recognise_words |>
group_by(book_exercise_name) |>
summarize(total = sum(n))
exercise_recognise_words <- left_join(exercise_recognise_words, total_words)
ggplot(exercise_recognise_words, aes(n/total, fill = recognise)) +
geom_histogram(alpha=0.5) +
facet_wrap("book_exercise_name", scales = "free") +
scale_fill_brewer(palette = "Dark2") +
scale_y_continuous(trans = scales::pseudo_log_trans()) +
labs(x = "Proportion of Total Words",
y = "Count (log10 scale)",
fill = "Recognised\nExercise?") +
theme_bw()
ggsave("term_frequency_exercise.png", width = 10, height = 5, dpi = 300)
# Zipfs law
freq_by_rank <- exercise_recognise_words |>
group_by(book_exercise_name, recognise) |>
mutate(rank = row_number(),
tf = n/total,
log10_rank = log10(rank),
log10_tf = log10(tf)) |>
ungroup()
lm <- lm(log10_tf ~ lspline(log10_rank, c(1)) * book_exercise_name * recognise, data = freq_by_rank)
zipf_preds <- predictions(lm,
variables = list(book_exercise_name = unique(freq_by_rank$book_exercise_name),
recognise = c("NO","YES")))
freq_by_rank |>
ggplot() +
geom_line(aes(x=rank, y=tf, color = recognise),
size = 1.1, alpha = 0.8, show.legend = FALSE) +
geom_ribbon(data = zipf_preds,
aes(x=10^log10_rank, ymin=10^conf.low, ymax=10^conf.high,
group = recognise),
alpha = 0.25) +
geom_line(data = zipf_preds,
aes(x=10^log10_rank, y=10^estimate, color = recognise),
linetype = "dashed") +
labs(x = "Term Frequency (proportion on log10 scale)",
y = "Term Rank (log10 scale)",
color = "Recognised\nExercise?") +
facet_wrap("book_exercise_name") +
scale_color_brewer(palette = "Dark2") +
scale_x_log10() +
scale_y_log10() +
theme_bw()
# inverse exercise (person) frequency
tf_ief <- exercise_words |>
bind_tf_idf(word, book_exercise_name, n)
tf_ief |>
select(-total) |>
arrange(desc(tf_idf))
library(forcats)
tf_ief |>
group_by(book_exercise_name) |>
top_n(5) |>
# ungroup() |>
# mutate(book_exercise_name = as.factor(book_exercise_name),
#        word = reorder_within(word, tf_idf, book_exercise_name)) |>
# separate(word, into = c("word", "lab")) |>
ggplot(aes(x=tf_idf,
y=reorder_within(word, tf_idf, book_exercise_name))) +
geom_col(show.legend = FALSE) +
labs(x = "Term Frequency - Inverse Exercise Frequency (tf-ief)", y = NULL) +
scale_y_reordered() +
facet_wrap(~book_exercise_name, scales = "free") +
theme_bw()
# plot relationships between word use across categorical variable
library(scales)
data_tokens |>
count(recognise, word) |>
group_by(recognise) |>
mutate(proportion = n / sum(n)) |>
select(-n) |>
pivot_wider(names_from = recognise, values_from = proportion) |>
ggplot(aes(x = NO, y = YES)) +
geom_abline(color = "gray40", lty = 2) +
geom_point(alpha = 0.1, size = 2.5) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_continuous(labels = percent_format()) +
scale_y_continuous(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001),
low = "darkslategray4", high = "gray75") +
theme(legend.position="none") +
theme_bw()
data_tokens |>
count(recognise, word) |>
group_by(recognise) |>
mutate(proportion = n / sum(n)) |>
select(-n) |>
pivot_wider(names_from = recognise, values_from = proportion) |>
ggplot(aes(x = NO, y = YES)) +
geom_abline(color = "gray40", lty = 2) +
geom_point(alpha = 0.1, size = 2.5) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_continuous(labels = percent_format()) +
scale_y_continuous(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001),
low = "darkslategray4", high = "gray75") +
theme(legend.position="none") +
facet_wrap(book_exercise_name)
data_tokens |>
count(recognise, word) |>
group_by(recognise) |>
mutate(proportion = n / sum(n)) |>
select(-n) |>
pivot_wider(names_from = recognise, values_from = proportion) |>
ggplot(aes(x = NO, y = YES)) +
geom_abline(color = "gray40", lty = 2) +
geom_point(alpha = 0.1, size = 2.5) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_continuous(labels = percent_format()) +
scale_y_continuous(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001),
low = "darkslategray4", high = "gray75") +
theme(legend.position="none") +
facet_wrap("book_exercise_name")
# A word cloud
library(wordcloud)
data_tokens |>
count(word) |>
with(wordcloud(word, n, max.words = 100))
data_tokens |>
filter(!is.na(book_exercise_name)) |>
count(word, book_exercise_name) |>
reshape2::acast(word ~ book_exercise_name, value.var = "n", fill = 0) |>
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 100)
# Common bigrams
data_bigrams <- data |>
select(ResponseId, book_exercise_name,
body_position, body_part, action, equipment, equipment_position, action_direction, misc,
recognise, response_name) |>
separate(response_name, into = c("word1", "word2"), sep = " ")
View(data_bigrams)
# Common bigrams
data_bigrams <- data |>
select(ResponseId, book_exercise_name,
body_position, body_part, action, equipment, equipment_position, action_direction, misc,
recognise, response_name) |>
unnest_tokens(word, response_name) |>
separate(bigram, into = c("word1", "word2"), sep = " ")
# Common bigrams
data_bigrams <- data |>
select(ResponseId, book_exercise_name,
body_position, body_part, action, equipment, equipment_position, action_direction, misc,
recognise, response_name) |>
unnest_tokens(bigram, response_name) |>
separate(bigram, into = c("word1", "word2"), sep = " ")
data_bigrams <- data |>
select(ResponseId, book_exercise_name,
body_position, body_part, action, equipment, equipment_position, action_direction, misc,
recognise, response_name) |>
unnest_tokens(bigram, response_name)
# Common bigrams
data_bigrams <- data |>
select(ResponseId, book_exercise_name,
body_position, body_part, action, equipment, equipment_position, action_direction, misc,
recognise, response_name) |>
unnest_tokens(bigram, response_name, token = "ngrams", n =2) |>
separate(bigram, into = c("word1", "word2"), sep = " ")
bigrams_filtered <- data_bigrams |>
filter(!word1 %in% stop_words$word) |>
filter(!word2 %in% stop_words$word)
# spelling errors - https://books.psychstat.org/textmining/data.html
words1 <- unique(data_bigrams$word1)
bad_words1 <- hunspell(words1)
bad_words1 <- unique(unlist(bad_words1))
suggest_words1 <- hunspell_suggest(bad_words1)
suggest_words1 <- unlist(lapply(suggest_words1, function(x) x[1]))
# # combine and compare suggestions (manually checked obvious errors)
#
bad_suggest_words1 <- bind_cols(bad_words1, suggest_words1)
count_words1 <- count(data_bigrams, word)
count_words1 <- count(data_bigrams, word1)
bad_suggest_words1 <- inner_join(count_words1, bad_suggest_words1, by = c(word1 = "...1"))
View(bad_suggest_words1)
# Recode the incorrect suggestions manually with more than 2 uses
# (manually editing original if obvious incorrect spelling)
suggest_words1 <- recode(suggest_words1,
"flye" = "fly",
"flue" = "fly",
"flues" = "fly",
"flyes" = "fly",
"pend lay" = "pendlay",
"probated" = "pronated",
"DEC" = "deck",
"insulated" = "supinated",
"hop" = "ohp",
"felt" = "delt",
"pull downs" = "pull down",
"antediluvian" = "vitruvian",
"devoid" = "deltoid",
"soles" = "soleus",
"flex or" = "flexor",
"trice" = "tricep",
"pendent" = "pendlay",
"resistivity" = "resistive",
"kinetics" = "isokinetic",
"fliers" = "fly",
"font" = "dont",
"selector" = "selectorized",
"cal" = "calf",
"dumbbells" = "dumbbell",
"flatcar" = "flat bar",
"gastronomic" = "gastrocnemius",
"overboard" = "hoverboard",
"playpen" = "pendlay",
"pen delay" = "pendlay",
"tr" = "trx",
"virtual" = "vitruvian",
"id" = "idk",
"Maxine" = "machine",
"precede" = "pec deck",
"raid" = "raise",
"pinnate" = "supinate",
"trapezes" = "trapezius",
"trapeziums" = "trapezius",
"barb" = "barbell",
"calf raises" = "calf raise",
"chestfuls" = "chest fly",
"extrasensory" = "extensor",
"extensions" = "extension",
"floors" = "flexor",
"gluten" = "gluteal",
"spelldown" = "lat pull down",
"ply" = "Olympic",
"peck" = "pec deck",
"peddle" = "pendlay",
"dependably" = "pendlay",
"pen lay" = "pendlay",
"preach" = "press",
"detonated" = "pronated",
"teases" = "seated",
"selector" = "selectorized",
"serrate" = "serratus",
"ottoman" = "zottman"
)
bad_whole_words1 <- paste0("\\b", bad_words1, "\\b")
data_bigrams$word1 <- stri_replace_all_regex(data_bigrams$word, bad_whole_words1, suggest_words1,
vectorize_all = FALSE)
data_bigrams$word1 <- stri_replace_all_regex(data_bigrams$word1, bad_whole_words1, suggest_words1,
vectorize_all = FALSE)
# for all double barrel terms split unnest again
data_bigrams_word1 <-  data_bigrams |>
unnest_tokens(word1, word1)
# spelling errors - https://books.psychstat.org/textmining/data.html
words2 <- unique(data_bigrams$word2)
bad_words2 <- hunspell(words2)
bad_words2 <- unique(unlist(bad_words2))
suggest_words2 <- hunspell_suggest(bad_words2)
suggest_words2 <- unlist(lapply(suggest_words2, function(x) x[1]))
# # combine and compare suggestions (manually checked obvious errors)
#
bad_suggest_words2 <- bind_cols(bad_words2, suggest_words2)
count_words2 <- count(data_bigrams, word2)
bad_suggest_words2 <- inner_join(count_words2, bad_suggest_words2, by = c(word2 = "...1"))
# Recode the incorrect suggestions manually with more than 2 uses
# (manually editing original if obvious incorrect spelling)
suggest_words2 <- recode(suggest_words2,
"flye" = "fly",
"flue" = "fly",
"flues" = "fly",
"flyes" = "fly",
"pend lay" = "pendlay",
"probated" = "pronated",
"DEC" = "deck",
"insulated" = "supinated",
"hop" = "ohp",
"felt" = "delt",
"pull downs" = "pull down",
"antediluvian" = "vitruvian",
"devoid" = "deltoid",
"soles" = "soleus",
"flex or" = "flexor",
"trice" = "tricep",
"pendent" = "pendlay",
"resistivity" = "resistive",
"kinetics" = "isokinetic",
"fliers" = "fly",
"font" = "dont",
"selector" = "selectorized",
"cal" = "calf",
"dumbbells" = "dumbbell",
"flatcar" = "flat bar",
"gastronomic" = "gastrocnemius",
"overboard" = "hoverboard",
"playpen" = "pendlay",
"pen delay" = "pendlay",
"tr" = "trx",
"virtual" = "vitruvian",
"id" = "idk",
"Maxine" = "machine",
"precede" = "pec deck",
"raid" = "raise",
"pinnate" = "supinate",
"trapezes" = "trapezius",
"trapeziums" = "trapezius",
"barb" = "barbell",
"calf raises" = "calf raise",
"chestfuls" = "chest fly",
"extrasensory" = "extensor",
"extensions" = "extension",
"floors" = "flexor",
"gluten" = "gluteal",
"spelldown" = "lat pull down",
"ply" = "Olympic",
"peck" = "pec deck",
"peddle" = "pendlay",
"dependably" = "pendlay",
"pen lay" = "pendlay",
"preach" = "press",
"detonated" = "pronated",
"teases" = "seated",
"selector" = "selectorized",
"serrate" = "serratus",
"ottoman" = "zottman"
)
bad_whole_words2 <- paste0("\\b", bad_words2, "\\b")
data_bigrams$word2 <- stri_replace_all_regex(data_bigrams$word2, bad_whole_words2, suggest_words2,
vectorize_all = FALSE)
# for all double barrel terms split unnest again
data_bigrams_word2 <-  data_bigrams |>
unnest_tokens(word2, word2)
View(data_bigrams_word1)
# Common bigrams
data_bigrams <- data |>
select(ResponseId, book_exercise_name,
body_position, body_part, action, equipment, equipment_position, action_direction, misc,
recognise, response_name) |>
unnest_tokens(bigram, response_name, token = "ngrams", n =2) |>
separate(bigram, into = c("word1", "word2"), sep = " ")
bigrams_filtered <- data_bigrams |>
filter(!word1 %in% stop_words$word) |>
filter(!word2 %in% stop_words$word)
# new bigram counts:
bigram_counts <- bigrams_filtered |>
count(word1, word2, sort = TRUE)
library(igraph)
bigram_graph <- bigram_counts |>
filter(n > 5) |>
graph_from_data_frame()
bigram_counts
data_bigrams <- data |>
select(ResponseId, book_exercise_name,
body_position, body_part, action, equipment, equipment_position, action_direction, misc,
recognise, response_name) |>
unnest_tokens(bigram, response_name, token = "ngrams", n =2)
View(data_bigrams)
# Common bigrams
data_bigrams <- data |>
select(ResponseId, book_exercise_name,
body_position, body_part, action, equipment, equipment_position, action_direction, misc,
recognise, response_name) |>
unnest_tokens(bigram, response_name, token = "ngrams", n =2) |>
separate(bigram, into = c("word1", "word2"), sep = " ") |>
filter(!word1 %in% stop_words$word) |>
filter(!word2 %in% stop_words$word)
# Common bigrams
data_bigrams <- data |>
select(ResponseId, book_exercise_name,
body_position, body_part, action, equipment, equipment_position, action_direction, misc,
recognise, response_name) |>
unnest_tokens(bigram, response_name, token = "ngrams", n =2) |>
separate(bigram, into = c("word1", "word2"), sep = " ") |>
filter(!word1 %in% stop_words$word) |>
filter(!word2 %in% stop_words$word) |>
filter(!is.na(word1))
# new bigram counts:
bigram_counts <- bigrams_filtered |>
count(word1, word2, sort = TRUE)
bigram_counts
# new bigram counts:
bigram_counts <- data_bigrams |>
count(word1, word2, sort = TRUE)
bigram_counts
library(igraph)
bigram_graph <- bigram_counts |>
filter(n > 5) |>
graph_from_data_frame()
library(ggraph)
set.seed(2017)
ggraph(bigram_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
set.seed(2020)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n+100), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_graph()
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n+100), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
facet_graph(book_exercise_name) +
theme_graph()
# new bigram counts:
bigram_counts <- data_bigrams |>
count(word1, word2, book_exercise_name, sort = TRUE)
bigram_counts
# new bigram counts:
bigram_counts <- data_bigrams |>
group_by(book_exercise_name) |>
count(word1, word2, sort = TRUE)
bigram_counts
# new bigram counts:
bigram_counts <- data_bigrams |>
group_by(book_exercise_name) |>
count(word1, word2, sort = TRUE)
library(igraph)
bigram_graph <- bigram_counts |>
filter(n > 5) |>
graph_from_data_frame()
library(ggraph)
set.seed(2017)
ggraph(bigram_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
set.seed(2020)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n+100), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
facet_graph(book_exercise_name) +
theme_graph()
bigram_graph
library(tidygraph)
bigram_graph <- bigram_counts |>
filter(n > 5) |>
as_tbl_graph()
bigram_graph
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n+100), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
facet_graph(book_exercise_name) +
theme_graph()
highschool
as_tbl_graph(highschool)
highschool
bigram_counts
bigram_graph
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n+100), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
facet_graph(name) +
theme_graph()
